---
id: TSK-004
title: Create Alembic migration for consolidation tables
status: pending
ticket_id: TKT-001
estimate: L
type: implementation
dependencies:
  depends_on:
    - TSK-001
    - TSK-002
    - TSK-003
  blocks: []
---

# TSK-004: Create Alembic migration for consolidation tables

## Objective

Create a comprehensive Alembic migration that sets up the database schema for the Agent Memory Consolidation System.

## Context

The migration must create the consolidated_memories and consolidation_jobs tables, add consolidation fields to task_memories, and create all necessary indexes for performance.

## Deliverables

- [ ] `backend/migrations/versions/{timestamp}_consolidation_tables.py` - Migration file

## Implementation Notes

```python
# backend/migrations/versions/{timestamp}_consolidation_tables.py
"""Create consolidation tables and enhance task_memories

Revision ID: consolidation_001
Revises: {previous_revision}
Create Date: 2025-01-08

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = 'consolidation_001'
down_revision = '{previous_revision}'  # Replace with actual previous revision
branch_labels = None
depends_on = None


def upgrade() -> None:
    # Create consolidated_memories table
    op.create_table(
        'consolidated_memories',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('project_id', sa.String(), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('memory_type', sa.String(length=50), nullable=False),
        sa.Column('abstraction_level', sa.Integer(), nullable=False),
        sa.Column('pattern_type', sa.String(length=50), nullable=True),
        sa.Column('confidence_score', sa.Float(), nullable=True),
        sa.Column('source_memory_ids', postgresql.ARRAY(sa.String()), nullable=False),
        sa.Column('embedding', postgresql.ARRAY(sa.Float(), dimensions=1), nullable=True),
        sa.Column('tags', postgresql.ARRAY(sa.String(length=50)), nullable=True),
        sa.Column('consolidation_method', sa.String(length=100), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('consolidated_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('source_count', sa.Integer(), nullable=False),
        sa.Column('reuse_count', sa.Integer(), nullable=False, server_default='0'),
        sa.CheckConstraint(
            'abstraction_level IN (1, 2, 3)',
            name='consolidated_memories_abstraction_check'
        ),
        sa.CheckConstraint(
            "memory_type IN ('error_fix', 'discovery', 'decision', 'learning', 'warning', 'codebase_knowledge')",
            name='consolidated_memories_type_check'
        ),
        sa.PrimaryKeyConstraint('id')
    )

    # Create consolidation_jobs table
    op.create_table(
        'consolidation_jobs',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('project_id', sa.String(), nullable=False),
        sa.Column('status', sa.String(length=50), nullable=False),
        sa.Column('trigger_type', sa.String(length=50), nullable=False),
        sa.Column('scope', postgresql.JSONB(), nullable=True, server_default='{}'),
        sa.Column('phases_completed', postgresql.ARRAY(sa.String()), nullable=True, server_default='[]'),
        sa.Column('metrics', postgresql.JSONB(), nullable=True, server_default='{}'),
        sa.Column('started_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('completed_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('duration_seconds', sa.Integer(), nullable=True),
        sa.Column('error_message', sa.Text(), nullable=True),
        sa.Column('retry_count', sa.Integer(), nullable=False, server_default='0'),
        sa.Column('max_retries', sa.Integer(), nullable=False, server_default='3'),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('triggered_by', sa.String(length=255), nullable=True),
        sa.CheckConstraint(
            "status IN ('pending', 'running', 'completed', 'failed', 'cancelled')",
            name='consolidation_jobs_status_check'
        ),
        sa.CheckConstraint(
            "trigger_type IN ('scheduled', 'manual', 'threshold')",
            name='consolidation_jobs_trigger_check'
        ),
        sa.PrimaryKeyConstraint('id')
    )

    # Add columns to task_memories
    op.add_column(
        'task_memories',
        sa.Column('consolidated_into_id', sa.String(), nullable=True)
    )
    op.add_column(
        'task_memories',
        sa.Column('is_consolidated', sa.Boolean(), nullable=False, server_default='false')
    )

    # Create indexes for consolidated_memories
    op.create_index('idx_cm_project', 'consolidated_memories', ['project_id'])
    op.create_index('idx_cm_abstraction', 'consolidated_memories', ['abstraction_level'])
    op.create_index('idx_cm_type', 'consolidated_memories', ['memory_type'])
    op.create_index('idx_cm_pattern', 'consolidated_memories', ['pattern_type'])
    op.create_index('idx_cm_created', 'consolidated_memories', ['consolidated_at'])

    # Create GIN indexes for arrays
    op.create_index('idx_cm_tags', 'consolidated_memories', ['tags'], postgresql_using='gin')
    op.create_index('idx_cm_source_ids', 'consolidated_memories', ['source_memory_ids'], postgresql_using='gin')

    # Create HNSW index for vector similarity (requires pgvector extension)
    # Note: This index is created separately in SQL to handle pgvector properly
    op.execute("""
        CREATE INDEX idx_cm_embedding
        ON consolidated_memories
        USING hnsw (embedding vector_cosine_ops)
        WHERE embedding IS NOT NULL
    """)

    # Create full-text search index
    op.execute("""
        ALTER TABLE consolidated_memories
        ADD COLUMN content_tsv tsvector
        GENERATED ALWAYS AS (to_tsvector('english', coalesce(content, ''))) STORED
    """)
    op.create_index('idx_cm_content_tsv', 'consolidated_memories', ['content_tsv'], postgresql_using='gin')

    # Create indexes for consolidation_jobs
    op.create_index('idx_cj_project_status', 'consolidation_jobs', ['project_id', 'status'])
    op.create_index('idx_cj_created', 'consolidation_jobs', ['created_at'])
    op.create_index('idx_cj_trigger_type', 'consolidation_jobs', ['trigger_type'])

    # Create indexes for task_memories consolidation fields
    op.create_index(
        'idx_tm_consolidated',
        'task_memories',
        ['is_consolidated'],
        postgresql_where=sa.text('is_consolidated = true')
    )
    op.create_index(
        'idx_tm_consolidated_into',
        'task_memories',
        ['consolidated_into_id'],
        postgresql_where=sa.text('consolidated_into_id IS NOT NULL')
    )


def downgrade() -> None:
    # Drop indexes
    op.drop_index('idx_tm_consolidated_into', table_name='task_memories')
    op.drop_index('idx_tm_consolidated', table_name='task_memories')

    op.drop_index('idx_cj_trigger_type', table_name='consolidation_jobs')
    op.drop_index('idx_cj_created', table_name='consolidation_jobs')
    op.drop_index('idx_cj_project_status', table_name='consolidation_jobs')

    op.drop_index('idx_cm_content_tsv', table_name='consolidated_memories')
    op.execute('DROP INDEX IF EXISTS idx_cm_embedding')
    op.drop_index('idx_cm_source_ids', table_name='consolidated_memories')
    op.drop_index('idx_cm_tags', table_name='consolidated_memories')
    op.drop_index('idx_cm_created', table_name='consolidated_memories')
    op.drop_index('idx_cm_pattern', table_name='consolidated_memories')
    op.drop_index('idx_cm_type', table_name='consolidated_memories')
    op.drop_index('idx_cm_abstraction', table_name='consolidated_memories')
    op.drop_index('idx_cm_project', table_name='consolidated_memories')

    # Drop columns from task_memories
    op.drop_column('task_memories', 'is_consolidated')
    op.drop_column('task_memories', 'consolidated_into_id')

    # Drop tables
    op.drop_table('consolidation_jobs')
    op.drop_table('consolidated_memories')
```

## Acceptance Criteria

- [ ] Migration file created with proper revision ID
- [ ] upgrade() function creates all tables correctly
- [ ] downgrade() function properly reverses changes
- [ ] All indexes created (including HNSW for pgvector)
- [ ] Check constraints added for enum validation
- [ ] content_tsv generated column added
- [ ] Migration runs successfully: `alembic upgrade head`
- [ ] Downgrade runs successfully: `alembic downgrade -1`

## Dependencies

**Depends On**: TSK-001, TSK-002, TSK-003 (Models must be defined first)
**Blocks**: None (migration enables other tasks)

## Verification

```bash
# Run migration
cd backend
alembic upgrade head

# Verify tables created
psql $DATABASE_URL -c "\dt consolidated_memories"
psql $DATABASE_URL -c "\dt consolidation_jobs"

# Verify indexes
psql $DATABASE_URL -c "\di idx_cm_*"

# Test downgrade
alembic downgrade -1
alembic upgrade head
```

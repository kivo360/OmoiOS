---
id: TSK-008
title: Implement ConsolidationEngine orchestration
status: pending
ticket_id: TKT-002
estimate: L
type: implementation
dependencies:
  depends_on:
    - TSK-001
    - TSK-002
    - TSK-004
  blocks:
    - TSK-009
---

# TSK-008: Implement ConsolidationEngine orchestration

## Objective

Implement the ConsolidationEngine that orchestrates the four-phase consolidation workflow: Pattern Extraction → Memory Merging → Hierarchy Creation → Playbook Integration.

## Context

The ConsolidationEngine is the central coordinator that runs each consolidation phase in sequence, handles errors gracefully, and tracks progress/metrics.

## Deliverables

- [ ] `backend/omoi_os/services/consolidation_engine.py` - Engine implementation

## Implementation Notes

```python
# backend/omoi_os/services/consolidation_engine.py
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import logging
import time

from sqlalchemy.orm import Session

from omoi_os.services.embedding import EmbeddingService
from omoi_os.models.consolidation_job import ConsolidationJob, JobStatus


@dataclass
class ConsolidationResult:
    """Result of a consolidation job."""
    job_id: str
    status: JobStatus
    phases_completed: List[str]
    metrics: Dict[str, Any]
    duration_seconds: int
    error_message: Optional[str] = None


class ConsolidationEngine:
    """
    Orchestrates the multi-phase consolidation workflow.

    Phases:
    1. Pattern Extraction
    2. Memory Merging
    3. Hierarchy Creation
    4. Playbook Integration
    """

    def __init__(
        self,
        db: Session,
        embedding_service: EmbeddingService,
        project_id: str,
        scope: Optional[Dict[str, Any]] = None,
    ):
        self.db = db
        self.embedding_service = embedding_service
        self.project_id = project_id
        self.scope = scope or {}
        self.logger = logging.getLogger(__name__)

    async def execute(self, job_id: str) -> ConsolidationResult:
        """
        Execute the full consolidation workflow.

        Args:
            job_id: The consolidation job ID for tracking

        Returns:
            ConsolidationResult with metrics and status
        """
        start_time = time.time()
        phases_completed = []
        metrics = {}
        error_message = None

        try:
            # Phase 1: Pattern Extraction
            self.logger.info(f"Job {job_id}: Starting pattern extraction")
            pattern_result = await self._run_pattern_extraction()
            phases_completed.append("pattern_extraction")
            metrics["patterns_extracted"] = len(getattr(pattern_result, 'patterns', []))
            self.logger.info(f"Job {job_id}: Extracted {metrics['patterns_extracted']} patterns")

            # Phase 2: Memory Merging
            self.logger.info(f"Job {job_id}: Starting memory merging")
            merge_result = await self._run_memory_merging()
            phases_completed.append("memory_merging")
            metrics["memories_merged"] = len(getattr(merge_result, 'merged_memories', []))
            self.logger.info(f"Job {job_id}: Merged {metrics['memories_merged']} memory groups")

            # Phase 3: Hierarchy Creation
            self.logger.info(f"Job {job_id}: Starting hierarchy creation")
            hierarchy_result = await self._run_hierarchy_creation()
            phases_completed.append("hierarchy_creation")
            metrics["principles_created"] = len(getattr(hierarchy_result, 'principles', []))
            self.logger.info(f"Job {job_id}: Created {metrics['principles_created']} principles")

            # Phase 4: Playbook Integration
            self.logger.info(f"Job {job_id}: Starting playbook integration")
            playbook_result = await self._run_playbook_integration()
            phases_completed.append("playbook_integration")
            metrics["playbook_entries_created"] = len(getattr(playbook_result, 'created_entries', []))
            metrics["playbook_entries_updated"] = len(getattr(playbook_result, 'updated_entries', []))
            self.logger.info(f"Job {job_id}: Created {metrics['playbook_entries_created']} playbook entries")

            duration = int(time.time() - start_time)

            # Total memories processed
            pattern_processed = getattr(pattern_result, 'memories_processed', 0)
            merge_processed = getattr(merge_result, 'memories_processed', 0)
            metrics["memories_processed"] = max(pattern_processed, merge_processed)

            return ConsolidationResult(
                job_id=job_id,
                status=JobStatus.COMPLETED,
                phases_completed=phases_completed,
                metrics=metrics,
                duration_seconds=duration
            )

        except Exception as e:
            self.logger.error(f"Job {job_id} failed: {e}", exc_info=True)
            duration = int(time.time() - start_time)

            return ConsolidationResult(
                job_id=job_id,
                status=JobStatus.FAILED,
                phases_completed=phases_completed,
                metrics=metrics,
                duration_seconds=duration,
                error_message=str(e)
            )

    async def _run_pattern_extraction(self):
        """Run pattern extraction phase."""
        # Import here to avoid circular dependency
        from omoi_os.services.consolidation.pattern_extractor import PatternExtractor

        extractor = PatternExtractor(self.db, self.embedding_service)
        return await extractor.extract(
            project_id=self.project_id,
            scope=self.scope
        )

    async def _run_memory_merging(self):
        """Run memory merging phase."""
        # Import here to avoid circular dependency
        from omoi_os.services.consolidation.memory_merger import MemoryMerger

        merger = MemoryMerger(self.db, self.embedding_service)
        return await merger.merge(
            project_id=self.project_id,
            scope=self.scope
        )

    async def _run_hierarchy_creation(self):
        """Run hierarchy creation phase."""
        # Import here to avoid circular dependency
        from omoi_os.services.consolidation.hierarchy_creator import HierarchyCreator

        creator = HierarchyCreator(self.db, self.embedding_service)
        # Get patterns from previous phase
        # This is a placeholder - actual implementation would retrieve patterns
        patterns = []
        merged_memories = []
        return await creator.create_hierarchy(patterns, merged_memories)

    async def _run_playbook_integration(self):
        """Run playbook integration phase."""
        # Import here to avoid circular dependency
        from omoi_os.services.consolidation.playbook_integrator import PlaybookIntegrator

        integrator = PlaybookIntegrator(self.db)
        # Get patterns and principles from previous phases
        # This is a placeholder
        return await integrator.integrate([], [], self.project_id)
```

## Acceptance Criteria

- [ ] ConsolidationEngine implemented with four-phase orchestration
- [ ] Each phase executes sequentially
- [ ] Phase failures don't crash entire job (caught and logged)
- [ ] Progress tracked in phases_completed list
- [ ] Metrics collected for each phase
- [ ] Duration calculated correctly
- [ ] Error handling with detailed error messages
- [ ] Returns ConsolidationResult with all required fields
- [ ] All tests pass

## Dependencies

**Depends On**: TSK-001, TSK-002, TSK-004 (Models and migration)
**Blocks**: TSK-009 (Worker calls engine.execute())

## Notes

- Phase services are imported lazily to avoid circular dependencies
- Each phase is wrapped in try/except to handle failures
- Metrics are accumulated across phases
- Logger provides detailed progress tracking

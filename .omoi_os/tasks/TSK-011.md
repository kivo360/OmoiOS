---
id: TSK-011
title: Implement ConsolidationService orchestrator
status: pending
ticket_id: TKT-002
estimate: M
type: implementation
dependencies:
  depends_on: [TSK-006, TSK-007, TSK-008, TSK-009, TSK-010]
  blocks: []
---

# TSK-011: Implement ConsolidationService orchestrator

## Objective

Implement the main ConsolidationService orchestrator that coordinates all consolidation operations.

## Context

The ConsolidationService is the main entry point for consolidation. It coordinates the sub-components (detector, merger, synthesizer, archiver, promoter) and manages the consolidation job lifecycle.

## Deliverables

- `backend/omoi_os/services/consolidation/consolidation_service.py` - ConsolidationService class

## Implementation Notes

```python
# omoi_os/services/consolidation/consolidation_service.py
from typing import List
from sqlalchemy.orm import Session

from omoi_os.services.memory import MemoryService
from omoi_os.services.embedding import EmbeddingService
from omoi_os.models.consolidation_job import ConsolidationJob
from omoi_os.models.consolidation_change import ConsolidationChange
from omoi_os.schemas.consolidation import (
    ConsolidationOptions,
    ConsolidationResult,
)

from omoi_os.services.consolidation.similarity_detector import SimilarityDetector
from omoi_os.services.consolidation.memory_merger import MemoryMerger
from omoi_os.services.consolidation.memory_synthesizer import MemorySynthesizer
from omoi_os.services.consolidation.memory_archiver import MemoryArchiver
from omoi_os.services.consolidation.playbook_promoter import PlaybookPromoter

class ConsolidationService:
    """
    Main orchestrator for memory consolidation.

    Coordinates:
    - Similarity detection and clustering
    - Memory merging
    - Summary synthesis
    - Memory archival
    - Playbook promotion
    - Audit logging
    """

    def __init__(
        self,
        memory_service: MemoryService,
        embedding_service: EmbeddingService,
    ):
        self.memory_service = memory_service
        self.embedding_service = embedding_service

        # Sub-components
        self.detector = SimilarityDetector(embedding_service)
        self.merger = MemoryMerger(embedding_service)
        self.synthesizer = MemorySynthesizer(embedding_service, llm_service)
        self.archiver = MemoryArchiver()
        self.promoter = PlaybookPromoter()

    async def execute_consolidation(
        self,
        session: Session,
        project_id: str,
        options: ConsolidationOptions,
        trigger_type: str,
        triggered_by: str,
    ) -> ConsolidationResult:
        """
        Execute full consolidation workflow.

        Steps:
        1. Create consolidation job
        2. Detect similar clusters
        3. Merge clusters (if enabled)
        4. Create summaries (if enabled)
        5. Archive stale memories (if enabled)
        6. Promote to playbook (if enabled)
        7. Record all changes
        8. Update job status

        Returns ConsolidationResult with all affected memory IDs.
        """
        # Create job
        job = ConsolidationJob(
            project_id=project_id,
            status="running",
            trigger_type=trigger_type,
            options=options.dict(),
            triggered_by=triggered_by,
            started_at=utc_now(),
        )
        session.add(job)
        session.flush()

        created_ids = []
        modified_ids = []
        archived_ids = []
        change_ids = []

        try:
            # 1. Detect clusters
            clusters = await self.detector.detect_clusters(
                session,
                project_id,
                options.similarity_threshold,
                options.min_cluster_size,
            )

            # 2. Merge clusters
            if options.merge:
                for cluster in clusters:
                    if len(cluster.memory_ids) >= options.min_cluster_size:
                        merged_id = await self.merger.merge_cluster(session, cluster)
                        created_ids.append(merged_id)

                        # Record change
                        change = self._record_change(
                            session, job.id, "merge", merged_id, cluster.memory_ids
                        )
                        change_ids.append(change.id)

                        job.memories_merged += 1

            # 3. Create summaries
            if options.summarize:
                for cluster in clusters:
                    if len(cluster.memory_ids) >= 3:  # Only summarize larger clusters
                        summary_id = await self.synthesizer.create_summary(
                            session, cluster
                        )
                        created_ids.append(summary_id)

                        change = self._record_change(
                            session, job.id, "summarize", summary_id, cluster.memory_ids
                        )
                        change_ids.append(change.id)

                        job.summaries_created += 1

                        # Promote if high confidence
                        if options.promote:
                            summary_memory = session.get(TaskMemory, summary_id)
                            if summary_memory.synthesis_confidence >= options.promote_confidence_threshold:
                                entry_id = await self.promoter.promote_to_playbook(
                                    session, summary_id, summary_memory.synthesis_confidence
                                )
                                if entry_id:
                                    job.memories_promoted += 1

            # 4. Archive stale memories
            if options.archive:
                archived = await self.archiver.archive_stale_memories(
                    session,
                    project_id,
                    options.archive_stale_days,
                    options.archive_min_reuse,
                )
                archived_ids.extend(archived)
                job.memories_archived += len(archived)

                for mem_id in archived:
                    change = self._record_change(
                        session, job.id, "archive", mem_id, []
                    )
                    change_ids.append(change.id)

            # Update job status
            job.status = "completed"
            job.completed_at = utc_now()
            session.flush()

            return ConsolidationResult(
                job_id=job.id,
                operation="consolidation",
                created_memory_ids=created_ids,
                modified_memory_ids=modified_ids,
                archived_memory_ids=archived_ids,
                input_count=len(clusters) * 2,  # Approximate
                output_count=len(created_ids),
                reduction_ratio=0.5,  # Calculate
                consolidation_change_ids=change_ids,
            )

        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            job.completed_at = utc_now()
            session.flush()
            raise

    def _record_change(
        self,
        session: Session,
        job_id: str,
        operation: str,
        memory_id: str,
        source_ids: List[str],
    ) -> ConsolidationChange:
        """Record a consolidation change in the audit log."""
        change = ConsolidationChange(
            consolidation_job_id=job_id,
            operation=operation,
            memory_id=memory_id,
            previous_state={"source_ids": source_ids},
            new_state={"consolidated": True},
            changed_by="system",
        )
        session.add(change)
        session.flush()
        return change
```

## Acceptance Criteria

- [ ] ConsolidationService class with execute_consolidation() method
- [ ] Coordinates all sub-components
- [ ] Creates and updates ConsolidationJob
- [ ] Records all ConsolidationChange entries
- [ ] Supports enabling/disabling operations via options
- [ ] Error handling with job status updates
- [ ] Returns ConsolidationResult with all affected IDs
- [ ] Integration tests for full workflow

## Dependencies

**Depends On**: TSK-006, TSK-007, TSK-008, TSK-009, TSK-010
**Blocks**: None

## Estimate

**Size**: M (2-4 hours)
